{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"!pip install swig\n!pip install gym[box2d]\n! pip install pyvirtualdisplay","metadata":{"execution":{"iopub.status.busy":"2024-02-06T22:56:30.530409Z","iopub.execute_input":"2024-02-06T22:56:30.530720Z","iopub.status.idle":"2024-02-06T22:57:50.796284Z","shell.execute_reply.started":"2024-02-06T22:56:30.530688Z","shell.execute_reply":"2024-02-06T22:57:50.795337Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting swig\n  Downloading swig-4.2.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.6 kB)\nDownloading swig-4.2.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: swig\nSuccessfully installed swig-4.2.0\nRequirement already satisfied: gym[box2d] in /opt/conda/lib/python3.10/site-packages (0.26.2)\nRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gym[box2d]) (1.24.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym[box2d]) (2.2.1)\nRequirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym[box2d]) (0.0.8)\nCollecting box2d-py==2.3.5 (from gym[box2d])\n  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting pygame==2.1.0 (from gym[box2d])\n  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: swig==4.* in /opt/conda/lib/python3.10/site-packages (from gym[box2d]) (4.2.0)\nBuilding wheels for collected packages: box2d-py\n  Building wheel for box2d-py (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=495182 sha256=4d73acda6c90b23687e31acacd272347e1fe9fc938125b6f31566b09d29f041d\n  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\nSuccessfully built box2d-py\nInstalling collected packages: box2d-py, pygame\nSuccessfully installed box2d-py-2.3.5 pygame-2.1.0\nCollecting pyvirtualdisplay\n  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\nInstalling collected packages: pyvirtualdisplay\nSuccessfully installed pyvirtualdisplay-3.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import gym\nfrom pyvirtualdisplay import Display\n\nimport os\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport PIL\nimport copy\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-02-06T22:57:50.798604Z","iopub.execute_input":"2024-02-06T22:57:50.799343Z","iopub.status.idle":"2024-02-06T22:57:55.848269Z","shell.execute_reply.started":"2024-02-06T22:57:50.799300Z","shell.execute_reply":"2024-02-06T22:57:55.847448Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Building the environment","metadata":{}},{"cell_type":"markdown","source":"## Testing the environment with random actions","metadata":{}},{"cell_type":"code","source":"# Set up a virtual display to render the Lunar Lander environment.\nDisplay(visible=0, size=(840, 480)).start();","metadata":{"execution":{"iopub.status.busy":"2024-02-06T22:57:58.638755Z","iopub.execute_input":"2024-02-06T22:57:58.639642Z","iopub.status.idle":"2024-02-06T22:57:59.043176Z","shell.execute_reply.started":"2024-02-06T22:57:58.639612Z","shell.execute_reply":"2024-02-06T22:57:59.042270Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"env = gym.make('LunarLander-v2', render_mode='rgb_array')\nenv.reset()\n\n# Play one complete episode with random actions\nwhile True:\n    action = env.action_space.sample()     \n    _, _, done, _, _ = env.step(action)\n    if done:\n        break\n\nprint(\"States size: \",str(env.observation_space.shape))\nprint(\"Number of actions: \",str(env.action_space.n))\nenv.reset()\nPIL.Image.fromarray(env.render())","metadata":{"execution":{"iopub.status.busy":"2024-02-06T23:16:47.404787Z","iopub.execute_input":"2024-02-06T23:16:47.405432Z","iopub.status.idle":"2024-02-06T23:16:47.450797Z","shell.execute_reply.started":"2024-02-06T23:16:47.405398Z","shell.execute_reply":"2024-02-06T23:16:47.449939Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"States size:  (8,)\nNumber of actions:  4\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n  if not isinstance(terminated, (bool, np.bool8)):\n","output_type":"stream"},{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"<PIL.Image.Image image mode=RGB size=600x400>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Possible actions","metadata":{}},{"cell_type":"code","source":"actions = {\n    0: \"do nothing\",\n    1: \"fire left orientation engine\",\n    2: \"fire main engine\",\n    3: \"fire right orientation engine\"\n    }","metadata":{"execution":{"iopub.status.busy":"2024-02-06T22:58:10.505784Z","iopub.execute_input":"2024-02-06T22:58:10.506171Z","iopub.status.idle":"2024-02-06T22:58:10.511444Z","shell.execute_reply.started":"2024-02-06T22:58:10.506134Z","shell.execute_reply":"2024-02-06T22:58:10.510263Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"## Dueling architecture + Double DQN algorithm","metadata":{}},{"cell_type":"code","source":"class D3DQN(nn.Module):\n    def __init__(self, input_observations, action_num, lr, chkpt_dir, name):\n        super(D3DQN, self).__init__()\n        # Save checkpoint path\n        self.chkpt_dir = chkpt_dir\n        self.checkpoint_file = os.path.join(self.chkpt_dir, name)\n        # Create the layers               \n        self.input = nn.Linear(input_observations, 256)\n        \n        # V stream\n        self.V = nn.Linear(256, 256)\n        self.V_output = nn.Linear(256,1)\n        \n        # A stream\n        self.A = nn.Linear(256, 256)\n        self.A_output = nn.Linear(256,action_num)\n        \n        # Optimizer\n        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n        self.loss = torch.nn.HuberLoss()\n        \n        # Set the GPU or CPU configuration for the network\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.to(self.device)\n        \n    def forward(self, obs_state):\n        \n        # Input stream\n        input = F.relu(self.input(obs_state))       \n        \n        # V stream\n        V = F.relu(self.V(input))\n        V = self.V_output(V)\n        \n        # A stream\n        A = F.relu(self.A(input))\n        A = self.A_output(A)\n        \n        return V, A\n    \n    def save_checkpoint(self):\n        print(\"... saving checkpoint ...\")\n        torch.save(self.state_dict(), self.checkpoint_file)\n        \n    def load_checkpoint(self):\n        print(\"... loading checkpoint ...\")\n        self.load_state_dict(torch.load(self.checkpoint_file))       ","metadata":{"execution":{"iopub.status.busy":"2024-02-06T23:18:40.530380Z","iopub.execute_input":"2024-02-06T23:18:40.531249Z","iopub.status.idle":"2024-02-06T23:18:40.542015Z","shell.execute_reply.started":"2024-02-06T23:18:40.531217Z","shell.execute_reply":"2024-02-06T23:18:40.541117Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"# Replay memory","metadata":{}},{"cell_type":"code","source":"class ReplayBuffer:\n\n    def __init__(self, max_size, pointer):\n        # Memory\n        self.storage = []\n        # Memory size\n        self.max_size = max_size\n        # Puntero a las diferentes celdas en la memoria\n        self.experience_ind = pointer        \n        self.full_storage_flag = False\n\n    def add(self, experience):\n        ######## Save experience         \n        \n        if self.experience_ind >= self.max_size:\n            self.full_storage_flag = True\n            self.experience_ind = 0                \n        if self.full_storage_flag:\n            self.storage[self.experience_ind] = experience\n        else:\n            self.storage.append(experience)        \n        \n        self.experience_ind += 1\n        print(\"EXP ADDED\")  \n\n    def sample(self, batch_size):\n        # Selecting the experience in memory\n        batch = np.random.choice(len(self.storage), batch_size, replace=False)\n        minibatch_rewards = []\n        minibatch_dones = []        \n        minibatch_current_state, minibatch_next_state = np.empty((batch_size, 4)), np.empty((batch_size, 4))\n\n        for i in batch:\n            ### Get data\n            # Getting current state            \n            minibatch_current_state[i, :, :] = self.storage[i][0]                                         \n            \n            # Getting next state\n            minibatch_next_state[i, :, :] = self.storage[i][3]                                         \n            \n            # Get reward           \n            minibatch_rewards.append(self.storage[i][1])                        \n            minibatch_dones.append(self.storage[i][4])\n            \n        return minibatch_current_state, np.array(minibatch_rewards), minibatch_next_state, np.array(minibatch_dones)","metadata":{"execution":{"iopub.status.busy":"2024-02-06T22:58:43.105871Z","iopub.execute_input":"2024-02-06T22:58:43.106621Z","iopub.status.idle":"2024-02-06T22:58:43.117671Z","shell.execute_reply.started":"2024-02-06T22:58:43.106590Z","shell.execute_reply":"2024-02-06T22:58:43.116513Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# D3DQN algorithm","metadata":{}},{"cell_type":"code","source":"class DRLAlgorithm:\n    def __init__(self):\n        \n        # Epsilon greedy policy\n        self.epsilon = 1.0\n        self.epsilon_final = 0.1\n        self.epsilon_interval = 1.0 - self.epsilon_final\n        self.epsilon_frames = 250_000\n        \n        # Network\n        self.q_net = D3DQN(\n            input_observations=8,\n            action_num=env.action_space.n,\n            lr=0.00025,\n            chkpt_dir=\"model\",\n            name = \"lunar_lander_model\"\n        )\n        self.q_target_net = copy.deepcopy(self.q_net) \n        self.batch_size = 3\n        \n        # Memory\n        self.memory = ReplayBuffer(500_000, 0)\n        self.replay_exp_initial_condition = 6\n    \n    def policy(self, state):\n        if np.random.rand() <= self.epsilon:\n            action = np.random.randint(0,env.action_space.n)\n            return\n                \n        # Expand dimensions \n        state_tensor = torch.tensor(state).to(q_net.device)\n        # Prediction\n        V, A = q_net.forward(state_tensor)       \n        A_mean = torch.mean(A)       \n        action = torch.argmax((V + (A - A_mean))).item()   \n        return action\n    \n    def train(self):\n        \n        if self.memory.experience_ind <= self.replay_exp_initial_condition:\n            return\n                \n        states_batch, rewards_batch, next_states_batch, dones_batch = self.memory.sample(self.batch_size)\n        \n        states = torch.tensor(states_batch).to(self.q_net.device)\n        next_states = torch.tensor(next_states_batch).to(self.q_net.device)\n        rewards = torch.tensor(rewards_batch).to(self.q_net.device)\n        dones = torch.tensor(dones_batch).to(self.q_net.device)\n        \n        self.q_net.optimizer.zero_grad()\n        \n        indices = np.arange(self.batch_size)\n        \n        V_s, A_s = self.q_net.forward(states)\n        V_s_target, A_s_target = self.q_target_net.forward(next_states)\n        \n        V_s_eval, A_s_eval = self.q_net.forward(next_states)\n        \n        \n                \n        \n        \n        \n        # Select the actions in each next states of the samples\n        next_actions = torch.argmax(self.q_net(next_states_batch)).item()\n        print(next_actions)\n        # Predictions with Q target network\n        q_next_preds = self.q_target_net(next_states_batch)        \n        \n        # Create variables for the next steps\n        q_target = np.empty_like(next_actions)               \n        \n        # Double DQN algorithm\n        for idx in range(dones_batch.shape[0]):\n            q_target_value = rewards_batch[idx]                     \n            if not dones_batch[idx]:                        \n                # If not done episode -> evaluate actions predicted\n                q_action_evaluated = q_next_preds[idx][next_actions[idx]]\n                # Bellman equation variation for Double DQN algorithm                                                \n                q_target_value += self.discount*q_action_evaluated                \n            # Final Q from Bellman Equation\n            q_target[idx] = q_target_value           \n        \n        print(\"Training... \")        \n        # Training network         \n        \n\n        \n        # Update epsilon\n         # Decay probability of taking random action\n        self.epsilon -= self.epsilon_interval / self.epsilon_greedy_frames\n        self.epsilon = max(self.epsilon, self.epsilon_final)\n        \n        self.update_network_counter += 1\n\n        if self.update_network_counter == 8000:\n            # Copy weights\n            self.q_target_net.load_state_dict(self.q_net.state_dict())\n            self.update_network_counter = 1\n\n        print(\"Training Finished\")\n        \n    def save_model(self):\n        self.q_net.save_checkpoint()\n    \n    def load_model(self):\n        self.q_net.load_checkpoint()\n        self.q_target_net = copy.deepcopy(self.q_net)\n        \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2024-02-06T23:46:46.698513Z","iopub.execute_input":"2024-02-06T23:46:46.698886Z","iopub.status.idle":"2024-02-06T23:46:46.706805Z","shell.execute_reply.started":"2024-02-06T23:46:46.698856Z","shell.execute_reply":"2024-02-06T23:46:46.705701Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"drl = DRLAlgorithm()","metadata":{"execution":{"iopub.status.busy":"2024-02-06T23:48:22.553628Z","iopub.execute_input":"2024-02-06T23:48:22.554002Z","iopub.status.idle":"2024-02-06T23:48:22.564279Z","shell.execute_reply.started":"2024-02-06T23:48:22.553976Z","shell.execute_reply":"2024-02-06T23:48:22.563375Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"markdown","source":"## Main","metadata":{}},{"cell_type":"code","source":"state = env.reset()\nstate = state[0]\nwhile True:\n    action = drl.policy(state)   \n    next_state, reward, done, _, _ = env.step(action)    \n    experience = [state, reward, action, next_state, done]\n    drl.memory.add(experience)\n    if done:\n        break","metadata":{"execution":{"iopub.status.busy":"2024-02-06T23:49:04.630312Z","iopub.execute_input":"2024-02-06T23:49:04.630700Z","iopub.status.idle":"2024-02-06T23:49:04.684252Z","shell.execute_reply.started":"2024-02-06T23:49:04.630670Z","shell.execute_reply":"2024-02-06T23:49:04.683336Z"},"trusted":true},"execution_count":115,"outputs":[{"name":"stdout","text":"EXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\nEXP ADDED\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-02-06T23:19:00.290784Z","iopub.execute_input":"2024-02-06T23:19:00.291154Z","iopub.status.idle":"2024-02-06T23:19:00.296866Z","shell.execute_reply.started":"2024-02-06T23:19:00.291125Z","shell.execute_reply":"2024-02-06T23:19:00.295713Z"},"trusted":true},"execution_count":86,"outputs":[]}]}